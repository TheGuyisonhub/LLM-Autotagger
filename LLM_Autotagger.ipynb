{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2T-Y0WLTXwj"
      },
      "outputs": [],
      "source": [
        "# üß† Auto Tagging Support Tickets Using LLM (Free Version)\n",
        "# -----------------------------------------------------------\n",
        "# Objective: Automatically tag support tickets into categories using LLMs (FREE)\n",
        "# Dataset: Free-text Support Ticket Dataset from Kaggle\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "# ‚úÖ Step 1: Kaggle API Setup\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Check if kaggle.json exists\n",
        "if not os.path.exists(\"/root/.kaggle/kaggle.json\"):\n",
        "    print(\"‚ö†Ô∏è kaggle.json not found. Please upload your Kaggle API key file.\")\n",
        "    uploaded = files.upload()  # Prompts user to upload kaggle.json\n",
        "    os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "    for fname in uploaded.keys():\n",
        "        os.rename(fname, \"/root/.kaggle/kaggle.json\")\n",
        "    os.chmod(\"/root/.kaggle/kaggle.json\", 0o600)\n",
        "    print(\"‚úÖ Kaggle API key saved successfully.\")\n",
        "else:\n",
        "    print(\"‚úÖ kaggle.json already exists. Continuing...\")\n",
        "\n",
        "# Test Kaggle API\n",
        "!kaggle datasets list -s \"support ticket\" | head -n 10\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ‚úÖ Step 2: Download Dataset\n",
        "!kaggle datasets download -d jutrera/customer-support-tickets -p /content --unzip\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ‚úÖ Step 3: Load & Inspect Dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/customer_support_tickets.csv')\n",
        "print(\"Rows:\", len(df))\n",
        "df.head()\n",
        "\n",
        "# Basic cleanup\n",
        "df = df.dropna(subset=['text', 'category'])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ‚úÖ Step 4: Zero-Shot Classification (Free via Hugging Face)\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "# Candidate labels ‚Äî take top 10 unique categories\n",
        "candidate_labels = list(df['category'].unique())[:10]\n",
        "\n",
        "# Test on few examples\n",
        "for i in range(3):\n",
        "    text = df['text'][i]\n",
        "    result = classifier(text, candidate_labels, multi_label=True)\n",
        "    top3 = list(zip(result['labels'][:3], result['scores'][:3]))\n",
        "    print(f\"\\nTICKET: {text}\\nTOP 3 TAGS:\")\n",
        "    for tag, score in top3:\n",
        "        print(f\"  - {tag} ({score:.2f})\")\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ‚úÖ Step 5: Fine-Tuning a Transformer Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Prepare data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['category'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Label encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_enc = label_encoder.fit_transform(train_labels)\n",
        "val_labels_enc = label_encoder.transform(val_labels)\n",
        "\n",
        "# Create dataset for Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    'text': list(train_texts) + list(val_texts),\n",
        "    'labels': list(train_labels_enc) + list(val_labels_enc)\n",
        "})\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
        "train_dataset = tokenized_dataset.select(range(len(train_texts)))\n",
        "val_dataset = tokenized_dataset.select(range(len(train_texts), len(dataset)))\n",
        "\n",
        "# Model setup\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# ‚úÖ Step 6: Evaluate and Output Top 3 Predictions\n",
        "import torch\n",
        "\n",
        "texts = val_texts[:5]\n",
        "inputs = tokenizer(list(texts), return_tensors='pt', padding=True, truncation=True)\n",
        "outputs = model(**inputs)\n",
        "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "for i, text in enumerate(texts):\n",
        "    top3 = torch.topk(probs[i], 3)\n",
        "    labels = [label_encoder.inverse_transform([idx.item()])[0] for idx in top3.indices]\n",
        "    scores = [round(val.item(), 2) for val in top3.values]\n",
        "    print(f\"\\nTICKET: {text}\\nTOP 3 TAGS:\")\n",
        "    for label, score in zip(labels, scores):\n",
        "        print(f\"  - {label} ({score})\")\n"
      ]
    }
  ]
}